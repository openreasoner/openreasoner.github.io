---
title: Reference
nav_order: 7
---




# Reference

## Inference-time Computing
[1][Alphazero-like tree-search can guide large language model decoding and training.](https://arxiv.org/pdf/2309.17179)

[2][Reasoning with language model is planning with world model.](https://arxiv.org/pdf/2305.14992)

[3][Scaling llm test-time compute optimally can be more effective than scaling model parameters](https://arxiv.org/pdf/2408.03314?)

[4][Think before you speak: Training language models with pause tokens](https://arxiv.org/pdf/2310.02226)


## From Outcome Supervision to Process Supervision

[1][Training verifiers to solve math word problems](https://arxiv.org/pdf/2110.14168)

[2][Solving math word problems with process-and
outcome-based feedback](https://arxiv.org/pdf/2211.14275)

[3][Letâ€™s verify step by step](https://arxiv.org/pdf/2305.20050)

[4][Making large language models better reasoners with step-aware verifier](https://arxiv.org/pdf/2206.02336)

[5][Ovm, outcome-supervised value models for planning in
mathematical reasoning](https://aclanthology.org/2024.findings-naacl.55.pdf)

[6][Generative verifiers: Reward modeling as next-token prediction](https://arxiv.org/pdf/2408.15240)

## Data Acquisition

[1][Star: Bootstrapping reasoning with reasoning](https://proceedings.neurips.cc/paper_files/paper/2022/file/639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf)

[2][Quiet-star: Language models can teach themselves to think before speaking](https://arxiv.org/pdf/2403.09629)

[3][Improve mathematical reasoning in language models by automated
process supervision](https://arxiv.org/pdf/2406.06592)

[4][Math-shepherd: Verify and reinforce llms step-by-step without human annotations](https://aclanthology.org/2024.acl-long.510.pdf)











